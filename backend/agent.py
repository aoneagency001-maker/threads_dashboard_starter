import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import re

from openai import OpenAI
from tqdm import tqdm
from . import parser as book_parser
from .smart_quote_extractor import SmartQuoteExtractor, QuoteQuality

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None


def _load_quotes(payload: Any) -> List[Dict[str, Any]]:
    if isinstance(payload, dict):
        return list(payload.get("quotes", []))
    if isinstance(payload, list):
        return payload
    return []


def _save_quotes(book: str, quotes: List[Dict[str, Any]], output_path: Path) -> None:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    payload = {"book": book, "quotes": quotes}
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def _refine_batch(quotes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """–£—Ç–æ—á–Ω—è–µ—Ç/–ø–æ–ª–∏—Ä—É–µ—Ç —É–∂–µ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Ü–∏—Ç–∞—Ç, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è.
    –í—Ö–æ–¥: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ (–∫–∞–∫ –º–∏–Ω–∏–º—É–º —Å –ø–æ–ª–µ–º quote). –í—ã—Ö–æ–¥: —Ç–æ—Ç –∂–µ —Ñ–æ—Ä–º–∞—Ç —Å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏.
    """
    if not quotes:
        return []

    # –õ–æ–∫–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∫ —Ñ–æ–ª–±—ç–∫
    def _local_polish(item: Dict[str, Any]) -> Dict[str, Any]:
        text = (item.get("quote") or item.get("translated") or item.get("original") or "").strip()
        text = re.sub(r"\s+", " ", text)[:250].strip()
        
        # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å
        if len(text) < 30:  # —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è
            return None
        if not text.endswith(('.', '!', '?', '‚Ä¶')):  # –Ω–µ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–∞—è –º—ã—Å–ª—å
            return None
        if any(bad in text.lower() for bad in ["scan to download", "www.", "http://", "https://", "–≥–ª–∞–≤–∞ ", "–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ"]):
            return None
            
        engaging = True if text else False
        meta = dict(item.get("meta") or {})
        meta["length"] = len(text)
        return {
            **item,
            "quote": text,
            "translated": item.get("translated") or text,
            "engaging": engaging,
            "category": item.get("category") or "",
            "style": item.get("style") or "insight",
            "meta": meta,
        }

    if client is None:
        return [it for it in [_local_polish(it) for it in quotes if (it.get("quote") or it.get("original"))] if it is not None]

    # –ì–æ—Ç–æ–≤–∏–º –±–∞—Ç—á –∫–∞–∫ JSON –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    system_prompt = (
        "–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ü–∏—Ç–∞—Ç. –ü—Ä–æ–≤–µ—Ä—å –∏ –æ—Ç–ø–æ–ª–∏—Ä—É–π —Ü–∏—Ç–∞—Ç—ã –¥–ª—è —Å–æ—Ü—Å–µ—Ç–µ–π.\n"
        "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ö–∞–∂–¥–∞—è —Ü–∏—Ç–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ü–û–õ–ù–û–°–¢–¨–Æ –û–°–ú–´–°–õ–ï–ù–ù–û–ô –∏ –ó–ê–í–ï–†–®–Å–ù–ù–û–ô –º—ã—Å–ª—å—é.\n"
        "–ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —Ü–∏—Ç–∞—Ç–∞:\n"
        "- —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—É—é –º—ã—Å–ª—å —Å –Ω–∞—á–∞–ª–æ–º –∏ –∫–æ–Ω—Ü–æ–º\n"
        "- –ø–æ–Ω—è—Ç–Ω–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
        "- –∏–º–µ–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è\n"
        "- –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–º –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n"
        "- –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è —Ç–æ—á–∫–æ–π, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–æ–º\n\n"
        "–î–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–π: –¥–ª–∏–Ω–∞ quote ‚â§ 250 —Å–∏–º–≤–æ–ª–æ–≤, engaging=true,\n"
        "–ø–æ–Ω—è—Ç–Ω–æ—Å—Ç—å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å—Å—ã–ª–æ–∫/–æ–≥–ª–∞–≤–ª–µ–Ω–∏–π/—Ä–µ–∫–ª–∞–º—ã.\n"
        "–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ JSON {quotes: [...]} –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, –ø–æ–ª—è: original, summary, quote, translated, engaging, category, style, meta{sentiment, target_audience, length}."
    )
    user_payload = {"quotes": [
        {
            "original": it.get("original", ""),
            "summary": it.get("summary", ""),
            "quote": (it.get("quote") or it.get("translated") or it.get("original") or ""),
            "translated": it.get("translated", ""),
            "engaging": True,
            "category": it.get("category", ""),
            "style": it.get("style", "insight"),
            "meta": it.get("meta", {}),
            "page": it.get("page"),
        }
        for it in quotes
    ]}
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
            ],
            temperature=0.2,
        )
        content = response.choices[0].message.content or "{}"
        data = json.loads(content) if content.strip().startswith("{") else {}
        items = data.get("quotes", []) if isinstance(data, dict) else []
        refined: List[Dict[str, Any]] = []
        for i, obj in enumerate(items):
            base = dict(quotes[i]) if i < len(quotes) else {}
            merged = _local_polish({**base, **obj})
            if merged and merged.get("quote"):
                refined.append(merged)
        return refined
    except Exception as e:
        print("–û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞:", e)
        return [it for it in [_local_polish(it) for it in quotes] if it is not None]


def refine_quotes(input_json_path: str, output_json_path: Optional[str] = None, batch_size: int = 30) -> str:
    """
    –ß–∏—Ç–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª —Ü–∏—Ç–∞—Ç, –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ—Ç –º—É—Å–æ—Ä –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–ª–µ "engaging"
    –∫ —Å–∏–ª—å–Ω—ã–º —Ü–∏—Ç–∞—Ç–∞–º, –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—è –∏—Ö –ø–æ–¥ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ
    –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É.
    """
    src = Path(input_json_path)
    if not src.exists():
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {src}")

    with open(src, "r", encoding="utf-8") as f:
        payload = json.load(f)

    book = payload.get("book") if isinstance(payload, dict) else src.stem
    quotes = _load_quotes(payload)
    if not quotes:
        # –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Å—Ç–æ–π
        out = Path(output_json_path) if output_json_path else src
        _save_quotes(str(book or ""), [], out)
        return str(out)

    refined_all: List[Dict[str, Any]] = []
    for i in tqdm(range(0, len(quotes), batch_size), desc="Refining", unit="batch"):
        batch = quotes[i : i + batch_size]
        refined = _refine_batch(batch)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ engaging=true
        refined_all.extend([it for it in refined if it.get("engaging") is True])

    # –¥–µ–¥—É–ø –ø–æ —Ç–µ–∫—Å—Ç—É —Ü–∏—Ç–∞—Ç—ã
    seen = set()
    deduped: List[Dict[str, Any]] = []
    for it in refined_all:
        key = (it.get("quote") or "").strip()
        if key and key not in seen:
            deduped.append(it)
            seen.add(key)

    out = Path(output_json_path) if output_json_path else src
    _save_quotes(str(book or ""), deduped, out)
    return str(out)


def _extract_engaging_from_chunk(chunk: str) -> List[Dict[str, Any]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 1‚Äì2 —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –∫—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ –Ω–æ–≤–æ–º—É —à–∞–±–ª–æ–Ω—É."""
    if not chunk.strip():
        return []

    # –§–æ–ª–±—ç–∫: —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ —Å –≥–ª–∞–≥–æ–ª–∞–º–∏ –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
    def heuristic_candidates(text: str) -> List[Dict[str, Any]]:
        terms = [
            "–≤–æ—Ä–æ–Ω–∫", "–∫–æ–Ω–≤–µ—Ä", "–ø—Ä–æ–¥–∞–∂", "–ª–∏–¥", "—Ç—Ä–∞—Ñ–∏–∫", "–∞—É–¥–∏—Ç–æ—Ä–∏", "–≤–Ω–∏–º–∞–Ω", "–æ—Ñ—Ñ–µ—Ä",
            "–º–∞—Ä–∫–µ—Ç", "–∑–∞–ø—É—Å–∫", "–ø—Ä–æ–¥—É–∫—Ç", "–≤–∏—Ä—É—Å", "–¥–æ—Ö–æ–¥", "–∫–ª–∏–µ–Ω—Ç", "—Ü–µ–Ω–Ω–æ—Å—Ç", "–æ–±–µ—â–∞–Ω",
        ]
        sents = re.split(r"(?<=[.!?])\s+", text)
        results: List[Dict[str, Any]] = []
        for s in sents:
            sent = s.strip()
            if len(sent) < 60:
                continue
            low = sent.lower()
            if not any(t in low for t in terms):
                continue
            if re.search(r"\b(–µ—Å—Ç—å|–¥–µ–ª–∞–π|–Ω—É–∂–Ω–æ|–¥–æ–ª–∂–µ–Ω|–º–æ–∂–Ω–æ|—Å—Ç—Ä–æ(–π|–∏—Ç—å)|–ø–æ–Ω–∏–º–∞–π|—Ç–µ—Å—Ç–∏—Ä—É–π|–∑–∞–ø—É—Å–∫–∞–π)\b", low):
                quote = sent[:250].strip()
                results.append({
                    "original": text,
                    "summary": "",
                    "quote": quote,
                    "translated": quote,
                    "engaging": True,
                    "category": "marketing",
                    "style": "insight",
                    "meta": {"sentiment": "motivational", "target_audience": "entrepreneurs, marketers", "length": len(quote)},
                })
            if len(results) >= 2:
                break
        return results

    if client is None:
        return heuristic_candidates(chunk)

    system_prompt = (
        "–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ü–∏—Ç–∞—Ç. \n"
        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏–∑ –∫–Ω–∏–≥–∏ –∏ –≤—ã–¥–µ–ª–∏ 1‚Äì2 –∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—è–º, –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∞–º –∏ —Å–æ–∑–¥–∞—Ç–µ–ª—è–º –æ–Ω–ª–∞–π–Ω-–∫—É—Ä—Å–æ–≤. \n"
        "–í–ê–ñ–ù–û: –ö–∞–∂–¥–∞—è —Ü–∏—Ç–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ü–û–õ–ù–û–°–¢–¨–Æ –û–°–ú–´–°–õ–ï–ù–ù–û–ô –∏ –ó–ê–í–ï–†–®–Å–ù–ù–û–ô –º—ã—Å–ª—å—é, –∞ –Ω–µ –æ–±—Ä—ã–≤–∫–æ–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n"
        "–ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —Ü–∏—Ç–∞—Ç–∞:\n"
        "- —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—É—é –º—ã—Å–ª—å —Å –Ω–∞—á–∞–ª–æ–º –∏ –∫–æ–Ω—Ü–æ–º\n"
        "- –ø–æ–Ω—è—Ç–Ω–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
        "- –∏–º–µ–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è\n"
        "- –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–º –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n\n"
        "–°—Ñ–æ—Ä–º–∏—Ä—É–π JSON {quotes: [ ... ]}, –≥–¥–µ –∫–∞–∂–¥–∞—è —Ü–∏—Ç–∞—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç:\n"
        "- original: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç\n"
        "- summary: –∫—Ä–∞—Ç–∫–∏–π —Å–º—ã—Å–ª\n"
        "- quote: –≤–æ–≤–ª–µ–∫–∞—é—â–∞—è —Ü–∏—Ç–∞—Ç–∞ (–ü–û–õ–ù–ê–Ø –∏ –û–°–ú–´–°–õ–ï–ù–ù–ê–Ø)\n"
        "- translated: –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\n"
        "- engaging: true\n"
        "- category: —Ç–µ–º–∞ (–º–∞—Ä–∫–µ—Ç–∏–Ω–≥, –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è, –ø—Ä–æ–¥–∞–∂–∏, –º—ã—à–ª–µ–Ω–∏–µ)\n"
        "- style: insight / rule / mistake / observation\n"
        "- meta: {sentiment, target_audience, length}\n\n"
        "–¶–∏—Ç–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞:\n"
        "- –±—ã—Ç—å –Ω–µ –¥–ª–∏–Ω–Ω–µ–µ 250 —Å–∏–º–≤–æ–ª–æ–≤,\n"
        "- –∑–≤—É—á–∞—Ç—å –∫–∞–∫ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –∏–Ω—Å–∞–π—Ç (–º–æ–∂–Ω–æ —Å —ç–º–æ–¥–∑–∏ üî•‚ö°Ô∏èüí°),\n"
        "- –±—ã—Ç—å –ø–æ–Ω—è—Ç–Ω–æ–π –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–Ω–∏–≥–∏,\n"
        "- –≤—ã–∑—ã–≤–∞—Ç—å —ç–º–æ—Ü–∏—é –∏–ª–∏ —É–∑–Ω–∞–≤–∞–Ω–∏–µ,\n"
        "- –±—ã—Ç—å –ó–ê–í–ï–†–®–Å–ù–ù–û–ô –º—ã—Å–ª—å—é, –∞ –Ω–µ –æ–±—Ä—ã–≤–∫–æ–º.\n\n"
        "–ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–π –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è, –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏, —Å—Å—ã–ª–∫–∏, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∞–≤—Ç–æ—Ä–æ–≤.\n"
        "–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ JSON —Å –∫–ª—é—á–æ–º quotes."
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": chunk[:6000]},
            ],
            temperature=0.3,
        )
        content = resp.choices[0].message.content or "{}"
        data = json.loads(content) if content.strip().startswith("{") else {}
        arr = data.get("quotes", []) if isinstance(data, dict) else []
        cleaned: List[Dict[str, Any]] = []
        for obj in arr:
            q = (obj.get("quote") or "").strip()
            if not q or len(q) > 250:
                continue
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å
            if len(q) < 30:  # —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è
                continue
            if not q.endswith(('.', '!', '?', '‚Ä¶')):  # –Ω–µ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–∞—è –º—ã—Å–ª—å
                continue
            bad = ["scan to download", "www.", "http://", "https://", "–≥–ª–∞–≤–∞ ", "–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ"]
            if any(k in q.lower() for k in bad):
                continue
            meta = obj.get("meta") or {}
            meta["length"] = len(q)
            cleaned.append({
                "original": obj.get("original") or chunk,
                "summary": obj.get("summary", ""),
                "quote": q,
                "translated": obj.get("translated") or q,
                "engaging": True,
                "category": obj.get("category", ""),
                "style": obj.get("style", "insight"),
                "meta": meta,
            })
        return cleaned[:2]
    except Exception as e:
        print("–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–∏—Ç–∞—Ç –∏–∑ –∫—É—Å–∫–∞:", e)
        return heuristic_candidates(chunk)


def harvest_all_from_pdf(pdf_path: str, output_json_path: Optional[str] = None, max_sentences_per_chunk: int = 5) -> str:
    """
    –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –∫–Ω–∏–≥–µ: –∏–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ JSON.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ü–∏—Ç–∞—Ç.
    """
    pages = book_parser.extract_pages_from_pdf(pdf_path)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
    smart_extractor = SmartQuoteExtractor()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
    text_chunks = []
    page_numbers = []
    
    for idx, page_text in enumerate(pages, start=1):
        cleaned_page = book_parser.clean_text(page_text)
        # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ –∞–±–∑–∞—Ü—ã –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        paragraphs = re.split(r'\n\s*\n', cleaned_page)
        for paragraph in paragraphs:
            if len(paragraph.strip()) > 100:  # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –∞–±–∑–∞—Ü—ã
                text_chunks.append(paragraph)
                page_numbers.append(idx)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
    collected = smart_extractor.extract_smart_quotes(text_chunks, page_numbers)
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –∫–∞–∫ fallback
    if not collected:
        print("–£–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–µ –Ω–∞—à–µ–ª —Ü–∏—Ç–∞—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –º–µ—Ç–æ–¥...")
        for idx, page_text in enumerate(pages, start=1):
            cleaned_page = book_parser.clean_text(page_text)
            for chunk in book_parser._chunk_paragraphs(cleaned_page, max_sentences=max_sentences_per_chunk):
                items = _extract_engaging_from_chunk(chunk)
                for it in items:
                    key = (it.get("quote") or "").strip()
                    if key:
                        collected.append({
                            **it,
                            "page": idx,
                        })

    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    book_title = Path(pdf_path).stem
    if output_json_path:
        out = Path(output_json_path)
    else:
        # –∫–ª–∞–¥—ë–º —Ä—è–¥–æ–º —Å –æ–±—ã—á–Ω—ã–º json –∏–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞ parser
        base_dir = Path(__file__).resolve().parents[1]
        quotes_dir = base_dir / "data" / "quotes"
        quotes_dir.mkdir(parents=True, exist_ok=True)
        out = quotes_dir / (Path(book_title).stem.replace(" ", "-") + ".json")
    _save_quotes(book_title, collected, out)
    return str(out)


def improve_existing_quotes(input_json_path: str, output_json_path: Optional[str] = None) -> str:
    """
    –£–ª—É—á—à–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ü–∏—Ç–∞—Ç—ã —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞.
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–π —Ü–∏—Ç–∞—Ç—ã.
    """
    src = Path(input_json_path)
    if not src.exists():
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {src}")

    with open(src, "r", encoding="utf-8") as f:
        payload = json.load(f)

    book = payload.get("book") if isinstance(payload, dict) else src.stem
    quotes = _load_quotes(payload)
    
    if not quotes:
        out = Path(output_json_path) if output_json_path else src
        _save_quotes(str(book or ""), [], out)
        return str(out)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
    smart_extractor = SmartQuoteExtractor()
    improved_quotes = []
    
    print(f"–£–ª—É—á—à–∞–µ–º {len(quotes)} —Ü–∏—Ç–∞—Ç...")
    
    for quote_data in tqdm(quotes, desc="–£–ª—É—á—à–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç"):
        original_text = quote_data.get("original", "")
        current_quote = quote_data.get("quote", "")
        page_num = quote_data.get("page")
        
        if not original_text or not current_quote:
            continue
            
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —Ü–∏—Ç–∞—Ç—É
        analyses = smart_extractor.analyze_paragraph(original_text, page_num)
        
        # –ò—â–µ–º –ª—É—á—à—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É
        best_analysis = None
        for analysis in analyses:
            if analysis.quality in [QuoteQuality.EXCELLENT, QuoteQuality.GOOD] and analysis.confidence > 0.6:
                if not best_analysis or analysis.confidence > best_analysis.confidence:
                    best_analysis = analysis
        
        if best_analysis:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Ü–∏—Ç–∞—Ç—É
            improved_quote = {
                **quote_data,
                "quote": best_analysis.text,
                "translated": best_analysis.text,  # –ü–æ–∫–∞ –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞
                "summary": best_analysis.summary,  # –ö—Ä–∞—Ç–∫–∞—è –∏–¥–µ—è
                "engaging": best_analysis.quality == QuoteQuality.EXCELLENT,
                "category": best_analysis.category,
                "meta": {
                    **quote_data.get("meta", {}),
                    "sentiment": best_analysis.sentiment,
                    "target_audience": best_analysis.target_audience,
                    "length": len(best_analysis.text),
                    "confidence": best_analysis.confidence,
                    "context_score": best_analysis.context_score,
                    "practical_value": best_analysis.practical_value,
                    "completeness": best_analysis.completeness,
                    "quote_type": best_analysis.quote_type.value,
                    "reasoning": best_analysis.reasoning,
                    "improved": True
                }
            }
            improved_quotes.append(improved_quote)
        else:
            # –û—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ü–∏—Ç–∞—Ç—É, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            improved_quote = {
                **quote_data,
                "meta": {
                    **quote_data.get("meta", {}),
                    "improved": False,
                    "reasoning": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª—É—á—à–µ–π –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã"
                }
            }
            improved_quotes.append(improved_quote)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_quotes = []
    seen_quotes = set()
    
    for quote in improved_quotes:
        quote_text = quote.get("quote", "").strip()
        if quote_text and quote_text not in seen_quotes:
            unique_quotes.append(quote)
            seen_quotes.add(quote_text)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    out = Path(output_json_path) if output_json_path else src
    _save_quotes(str(book or ""), unique_quotes, out)
    
    print(f"‚úÖ –£–ª—É—á—à–µ–Ω–æ {len(unique_quotes)} —Ü–∏—Ç–∞—Ç. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {out}")
    return str(out)


