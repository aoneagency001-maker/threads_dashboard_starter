import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import re

from openai import OpenAI
from tqdm import tqdm
from . import parser as book_parser
from .smart_quote_extractor import SmartQuoteExtractor, QuoteQuality
from .quote_validator import QuoteValidator
from .gemini_extractor import GeminiDeepExtractor
from .claude_client import (
    is_claude_available,
    claude_refine_quotes,
    claude_extract_from_chunk,
    claude_analyze_quality
)

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None


def _load_quotes(payload: Any) -> List[Dict[str, Any]]:
    if isinstance(payload, dict):
        return list(payload.get("quotes", []))
    if isinstance(payload, list):
        return payload
    return []


def _save_quotes(book: str, quotes: List[Dict[str, Any]], output_path: Path) -> None:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    payload = {"book": book, "quotes": quotes}
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def _refine_batch(quotes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """–£—Ç–æ—á–Ω—è–µ—Ç/–ø–æ–ª–∏—Ä—É–µ—Ç —É–∂–µ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Ü–∏—Ç–∞—Ç, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è.
    –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è Threads (–¥–æ 500 —Å–∏–º–≤–æ–ª–æ–≤).
    –í—Ö–æ–¥: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ (–∫–∞–∫ –º–∏–Ω–∏–º—É–º —Å –ø–æ–ª–µ–º quote). –í—ã—Ö–æ–¥: —Ç–æ—Ç –∂–µ —Ñ–æ—Ä–º–∞—Ç —Å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏.
    """
    if not quotes:
        return []

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
    validator = QuoteValidator(use_ai=False)

    # –õ–æ–∫–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∫ —Ñ–æ–ª–±—ç–∫
    def _local_polish(item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        text = (item.get("quote") or item.get("translated") or item.get("original") or "").strip()
        text = re.sub(r"\s+", " ", text).strip()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –≤–º–µ—Å—Ç–æ —Ä—É—á–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        temp_quote_data = {
            **item,
            "quote": text
        }

        validated = validator.get_validated_quote(temp_quote_data)
        if validated is None:
            return None

        # –î–æ–ø–æ–ª–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        meta = dict(validated.get("meta") or {})
        meta["length"] = len(validated["quote"])

        return {
            **validated,
            "engaging": True,  # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é, —Å—á–∏—Ç–∞–µ–º engaging
            "category": validated.get("category") or "",
            "style": validated.get("style") or "insight",
            "meta": meta,
        }

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Claude –¥–ª—è –ø–æ–ª–∏—Ä–æ–≤–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω), –∏–Ω–∞—á–µ fallback –Ω–∞ GPT –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
    if is_claude_available():
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Claude –¥–ª—è –ø–æ–ª–∏—Ä–æ–≤–∫–∏
            refined_quotes = claude_refine_quotes(quotes)
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
            result = []
            for quote in refined_quotes:
                validated = _local_polish(quote)
                if validated and validated.get("quote"):
                    result.append(validated)
            return result if result else [it for it in [_local_polish(it) for it in quotes if (it.get("quote") or it.get("original"))] if it is not None]
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Claude –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback: {e}")
            # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
            return [it for it in [_local_polish(it) for it in quotes if (it.get("quote") or it.get("original"))] if it is not None]
    
    if client is None:
        return [it for it in [_local_polish(it) for it in quotes if (it.get("quote") or it.get("original"))] if it is not None]

    # Fallback –Ω–∞ GPT (–µ—Å–ª–∏ Claude –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ GPT –¥–æ—Å—Ç—É–ø–µ–Ω)
    # –ì–æ—Ç–æ–≤–∏–º –±–∞—Ç—á –∫–∞–∫ JSON –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    system_prompt = (
        "–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ü–∏—Ç–∞—Ç –¥–ª—è Threads (Instagram). –ü—Ä–æ–≤–µ—Ä—å –∏ –æ—Ç–ø–æ–ª–∏—Ä—É–π —Ü–∏—Ç–∞—Ç—ã.\n\n"
        "üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n"
        "1. –ó–ê–í–ï–†–®–ï–ù–ù–û–°–¢–¨: –ö–∞–∂–¥–∞—è —Ü–∏—Ç–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ü–û–õ–ù–û–°–¢–¨–Æ –û–°–ú–´–°–õ–ï–ù–ù–û–ô –∏ –ó–ê–í–ï–†–®–Å–ù–ù–û–ô –º—ã—Å–ª—å—é\n"
        "2. –î–õ–ò–ù–ê: –ú–∞–∫—Å–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤ (–ª–∏–º–∏—Ç Threads), –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ 100-400\n"
        "3. –ê–í–¢–û–ù–û–ú–ù–û–°–¢–¨: –ü–æ–Ω—è—Ç–Ω–∞ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–Ω–∏–≥–∏\n"
        "4. –¶–ï–ù–ù–û–°–¢–¨: –ò–º–µ–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª—å–∑—É –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è\n\n"
        "‚úÖ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ü–∏—Ç–∞—Ç–µ:\n"
        "- –°–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—É—é –º—ã—Å–ª—å —Å –Ω–∞—á–∞–ª–æ–º –∏ –∫–æ–Ω—Ü–æ–º\n"
        "- –ù–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–º –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n"
        "- –ó–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è —Ç–æ—á–∫–æ–π, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–æ–º\n"
        "- –ù–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Å—ã–ª–æ–∫, –æ–≥–ª–∞–≤–ª–µ–Ω–∏–π, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\n"
        "- –ò–º–µ–µ—Ç –º–∏–Ω–∏–º—É–º 5 –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤\n\n"
        "üìè –ö–æ–Ω—Ç—Ä–æ–ª—å –¥–ª–∏–Ω—ã:\n"
        "- –ï—Å–ª–∏ —Ü–∏—Ç–∞—Ç–∞ >500 —Å–∏–º–≤–æ–ª–æ–≤: —Å–æ–∫—Ä–∞—Ç–∏ –¥–æ –æ–¥–Ω–æ–≥–æ-–¥–≤—É—Ö –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n"
        "- –°–æ—Ö—Ä–∞–Ω—è–π —Å–∞–º—É—é —Ü–µ–Ω–Ω—É—é –º—ã—Å–ª—å\n"
        "- –ù–µ –æ–±—Ä—ã–≤–∞–π –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n\n"
        "–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ JSON {quotes: [...]} –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ.\n"
        "–ü–æ–ª—è: original, summary, quote (‚â§500 chars!), translated, engaging, category, style, meta{sentiment, target_audience, length}."
    )
    user_payload = {"quotes": [
        {
            "original": it.get("original", ""),
            "summary": it.get("summary", ""),
            "quote": (it.get("quote") or it.get("translated") or it.get("original") or ""),
            "translated": it.get("translated", ""),
            "engaging": True,
            "category": it.get("category", ""),
            "style": it.get("style", "insight"),
            "meta": it.get("meta", {}),
            "page": it.get("page"),
        }
        for it in quotes
    ]}
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
            ],
            temperature=0.2,
        )
        content = response.choices[0].message.content or "{}"
        data = json.loads(content) if content.strip().startswith("{") else {}
        items = data.get("quotes", []) if isinstance(data, dict) else []
        refined: List[Dict[str, Any]] = []
        for i, obj in enumerate(items):
            base = dict(quotes[i]) if i < len(quotes) else {}
            merged = _local_polish({**base, **obj})
            if merged and merged.get("quote"):
                refined.append(merged)
        return refined
    except Exception as e:
        print("–û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞:", e)
        return [it for it in [_local_polish(it) for it in quotes] if it is not None]


def refine_quotes(input_json_path: str, output_json_path: Optional[str] = None, batch_size: int = 30) -> str:
    """
    –ß–∏—Ç–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª —Ü–∏—Ç–∞—Ç, –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ—Ç –º—É—Å–æ—Ä –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–ª–µ "engaging"
    –∫ —Å–∏–ª—å–Ω—ã–º —Ü–∏—Ç–∞—Ç–∞–º, –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—è –∏—Ö –ø–æ–¥ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ
    –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É.
    """
    src = Path(input_json_path)
    if not src.exists():
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {src}")

    with open(src, "r", encoding="utf-8") as f:
        payload = json.load(f)

    book = payload.get("book") if isinstance(payload, dict) else src.stem
    quotes = _load_quotes(payload)
    if not quotes:
        # –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Å—Ç–æ–π
        out = Path(output_json_path) if output_json_path else src
        _save_quotes(str(book or ""), [], out)
        return str(out)

    refined_all: List[Dict[str, Any]] = []
    for i in tqdm(range(0, len(quotes), batch_size), desc="Refining", unit="batch"):
        batch = quotes[i : i + batch_size]
        refined = _refine_batch(batch)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ engaging=true
        refined_all.extend([it for it in refined if it.get("engaging") is True])

    # –¥–µ–¥—É–ø –ø–æ —Ç–µ–∫—Å—Ç—É —Ü–∏—Ç–∞—Ç—ã
    seen = set()
    deduped: List[Dict[str, Any]] = []
    for it in refined_all:
        key = (it.get("quote") or "").strip()
        if key and key not in seen:
            deduped.append(it)
            seen.add(key)

    out = Path(output_json_path) if output_json_path else src
    _save_quotes(str(book or ""), deduped, out)
    return str(out)


def _extract_engaging_from_chunk(chunk: str) -> List[Dict[str, Any]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 1‚Äì2 —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –∫—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ –Ω–æ–≤–æ–º—É —à–∞–±–ª–æ–Ω—É."""
    if not chunk.strip():
        return []

    # –§–æ–ª–±—ç–∫: —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ —Å –≥–ª–∞–≥–æ–ª–∞–º–∏ –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
    def heuristic_candidates(text: str) -> List[Dict[str, Any]]:
        terms = [
            "–≤–æ—Ä–æ–Ω–∫", "–∫–æ–Ω–≤–µ—Ä", "–ø—Ä–æ–¥–∞–∂", "–ª–∏–¥", "—Ç—Ä–∞—Ñ–∏–∫", "–∞—É–¥–∏—Ç–æ—Ä–∏", "–≤–Ω–∏–º–∞–Ω", "–æ—Ñ—Ñ–µ—Ä",
            "–º–∞—Ä–∫–µ—Ç", "–∑–∞–ø—É—Å–∫", "–ø—Ä–æ–¥—É–∫—Ç", "–≤–∏—Ä—É—Å", "–¥–æ—Ö–æ–¥", "–∫–ª–∏–µ–Ω—Ç", "—Ü–µ–Ω–Ω–æ—Å—Ç", "–æ–±–µ—â–∞–Ω",
        ]
        sents = re.split(r"(?<=[.!?])\s+", text)
        results: List[Dict[str, Any]] = []
        for s in sents:
            sent = s.strip()
            if len(sent) < 60:
                continue
            low = sent.lower()
            if not any(t in low for t in terms):
                continue
            if re.search(r"\b(–µ—Å—Ç—å|–¥–µ–ª–∞–π|–Ω—É–∂–Ω–æ|–¥–æ–ª–∂–µ–Ω|–º–æ–∂–Ω–æ|—Å—Ç—Ä–æ(–π|–∏—Ç—å)|–ø–æ–Ω–∏–º–∞–π|—Ç–µ—Å—Ç–∏—Ä—É–π|–∑–∞–ø—É—Å–∫–∞–π)\b", low):
                quote = sent[:250].strip()
                results.append({
                    "original": text,
                    "summary": "",
                    "quote": quote,
                    "translated": quote,
                    "engaging": True,
                    "category": "marketing",
                    "style": "insight",
                    "meta": {"sentiment": "motivational", "target_audience": "entrepreneurs, marketers", "length": len(quote)},
                })
            if len(results) >= 2:
                break
        return results

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Claude –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    if is_claude_available():
        try:
            quotes = claude_extract_from_chunk(chunk)
            if quotes:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞–∂–¥–æ–π —Ü–∏—Ç–∞—Ç—ã
                validator = QuoteValidator(use_ai=False)
                cleaned: List[Dict[str, Any]] = []
                
                for obj in quotes:
                    q = (obj.get("quote") or "").strip()
                    if not q:
                        continue
                    
                    quote_data = {
                        "original": obj.get("original") or chunk,
                        "summary": obj.get("summary", ""),
                        "quote": q,
                        "translated": obj.get("translated") or q,
                        "engaging": True,
                        "category": obj.get("category", ""),
                        "style": obj.get("style", "insight"),
                        "meta": obj.get("meta") or {},
                    }
                    
                    validated = validator.get_validated_quote(quote_data)
                    if validated:
                        cleaned.append(validated)
                
                return cleaned[:2]
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Claude –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback: {e}")
            # Fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É –∏–ª–∏ GPT
    
    if client is None:
        return heuristic_candidates(chunk)

    # Fallback –Ω–∞ GPT (–µ—Å–ª–∏ Claude –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
    system_prompt = (
        "–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ü–∏—Ç–∞—Ç –¥–ª—è Threads.\n\n"
        "üéØ –ó–ê–î–ê–ß–ê: –ò–∑–≤–ª–µ–∫–∏ 1-2 –∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π –∏ –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–æ–≤.\n\n"
        "üî• –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ú–ù–û–ì–û–≠–¢–ê–ü–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê:\n\n"
        "–≠–¢–ê–ü 1 - –ó–ê–í–ï–†–®–ï–ù–ù–û–°–¢–¨:\n"
        "- –¶–∏—Ç–∞—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –ü–û–õ–ù–£–Æ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—É—é –º—ã—Å–ª—å\n"
        "- –ü–æ–Ω—è—Ç–Ω–∞ –ë–ï–ó –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–Ω–∏–≥–∏\n"
        "- –ò–º–µ–µ—Ç –Ω–∞—á–∞–ª–æ –∏ –ª–æ–≥–∏—á–Ω—ã–π –∫–æ–Ω–µ—Ü\n"
        "- –ù–ï —è–≤–ª—è–µ—Ç—Å—è –æ–±—Ä—ã–≤–∫–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n"
        "- –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ç–æ—á–∫–æ–π/!/?\n\n"
        "–≠–¢–ê–ü 2 - –î–õ–ò–ù–ê –î–õ–Ø THREADS:\n"
        "- –ú–∞–∫—Å–∏–º—É–º: 500 —Å–∏–º–≤–æ–ª–æ–≤ (–∂–µ—Å—Ç–∫–∏–π –ª–∏–º–∏—Ç Threads)\n"
        "- –û–ø—Ç–∏–º–∞–ª—å–Ω–æ: 100-400 —Å–∏–º–≤–æ–ª–æ–≤\n"
        "- –ï—Å–ª–∏ –¥–ª–∏–Ω–Ω–µ–µ: –æ—Å—Ç–∞–≤—å 1-2 —Å–∞–º—ã—Ö —Ü–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n"
        "- –ù–ï –æ–±—Ä—ã–≤–∞–π –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ –º—ã—Å–ª–∏\n\n"
        "–≠–¢–ê–ü 3 - –û–°–ú–´–°–õ–ï–ù–ù–û–°–¢–¨:\n"
        "- –ú–∏–Ω–∏–º—É–º 5 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n"
        "- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è\n"
        "- –í—ã–∑—ã–≤–∞–µ—Ç —ç–º–æ—Ü–∏—é –∏–ª–∏ —É–∑–Ω–∞–≤–∞–Ω–∏–µ\n"
        "- –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —ç–º–æ–¥–∑–∏ –¥–ª—è –≤–æ–≤–ª–µ—á–µ–Ω–∏—è üî•‚ö°Ô∏èüí°\n\n"
        "–≠–¢–ê–ü 4 - –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê:\n"
        "- –ù–µ—Ç —Å—Å—ã–ª–æ–∫, –æ–≥–ª–∞–≤–ª–µ–Ω–∏–π, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\n"
        "- –ù–µ—Ç –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–µ–π, —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∞–≤—Ç–æ—Ä–æ–≤\n"
        "- –ù–µ—Ç –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤\n\n"
        "üìä JSON —Ñ–æ—Ä–º–∞—Ç {quotes: [...]}, –ø–æ–ª—è:\n"
        "- original: –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç\n"
        "- summary: —Å—É—Ç—å –≤ 1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏\n"
        "- quote: –≥–æ—Ç–æ–≤–∞—è —Ü–∏—Ç–∞—Ç–∞ (‚â§500 —Å–∏–º–≤–æ–ª–æ–≤!)\n"
        "- translated: –ø–µ—Ä–µ–≤–æ–¥\n"
        "- engaging: true\n"
        "- category: –º–∞—Ä–∫–µ—Ç–∏–Ω–≥/–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è/–ø—Ä–æ–¥–∞–∂–∏/–º—ã—à–ª–µ–Ω–∏–µ\n"
        "- style: insight/rule/mistake/observation\n"
        "- meta: {sentiment, target_audience, length}\n\n"
        "–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ JSON —Å –∫–ª—é—á–æ–º quotes."
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": chunk[:6000]},
            ],
            temperature=0.3,
        )
        content = resp.choices[0].message.content or "{}"
        data = json.loads(content) if content.strip().startswith("{") else {}
        arr = data.get("quotes", []) if isinstance(data, dict) else []

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞–∂–¥–æ–π —Ü–∏—Ç–∞—Ç—ã
        validator = QuoteValidator(use_ai=False)
        cleaned: List[Dict[str, Any]] = []

        for obj in arr:
            q = (obj.get("quote") or "").strip()
            if not q:
                continue

            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            quote_data = {
                "original": obj.get("original") or chunk,
                "summary": obj.get("summary", ""),
                "quote": q,
                "translated": obj.get("translated") or q,
                "engaging": True,
                "category": obj.get("category", ""),
                "style": obj.get("style", "insight"),
                "meta": obj.get("meta") or {},
            }

            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
            validated = validator.get_validated_quote(quote_data)
            if validated:
                cleaned.append(validated)

        return cleaned[:2]
    except Exception as e:
        print("–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–∏—Ç–∞—Ç –∏–∑ –∫—É—Å–∫–∞:", e)
        return heuristic_candidates(chunk)


def harvest_all_from_pdf(pdf_path: str, output_json_path: Optional[str] = None, max_sentences_per_chunk: int = 5) -> str:
    """
    –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –∫–Ω–∏–≥–µ: –∏–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ JSON.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ü–∏—Ç–∞—Ç.
    """
    pages = book_parser.extract_pages_from_pdf(pdf_path)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
    smart_extractor = SmartQuoteExtractor()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
    text_chunks = []
    page_numbers = []
    
    for idx, page_text in enumerate(pages, start=1):
        cleaned_page = book_parser.clean_text(page_text)
        # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ –∞–±–∑–∞—Ü—ã –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        paragraphs = re.split(r'\n\s*\n', cleaned_page)
        for paragraph in paragraphs:
            if len(paragraph.strip()) > 100:  # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –∞–±–∑–∞—Ü—ã
                text_chunks.append(paragraph)
                page_numbers.append(idx)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
    collected = smart_extractor.extract_smart_quotes(text_chunks, page_numbers)
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –∫–∞–∫ fallback
    if not collected:
        print("–£–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–µ –Ω–∞—à–µ–ª —Ü–∏—Ç–∞—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –º–µ—Ç–æ–¥...")
        for idx, page_text in enumerate(pages, start=1):
            cleaned_page = book_parser.clean_text(page_text)
            for chunk in book_parser._chunk_paragraphs(cleaned_page, max_sentences=max_sentences_per_chunk):
                items = _extract_engaging_from_chunk(chunk)
                for it in items:
                    key = (it.get("quote") or "").strip()
                    if key:
                        collected.append({
                            **it,
                            "page": idx,
                        })

    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    book_title = Path(pdf_path).stem
    if output_json_path:
        out = Path(output_json_path)
    else:
        # –∫–ª–∞–¥—ë–º —Ä—è–¥–æ–º —Å –æ–±—ã—á–Ω—ã–º json –∏–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞ parser
        base_dir = Path(__file__).resolve().parents[1]
        quotes_dir = base_dir / "data" / "quotes"
        quotes_dir.mkdir(parents=True, exist_ok=True)
        out = quotes_dir / (Path(book_title).stem.replace(" ", "-") + ".json")
    _save_quotes(book_title, collected, out)
    return str(out)


def improve_existing_quotes(input_json_path: str, output_json_path: Optional[str] = None) -> str:
    """
    –£–ª—É—á—à–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ü–∏—Ç–∞—Ç—ã —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞.
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–π —Ü–∏—Ç–∞—Ç—ã.
    """
    src = Path(input_json_path)
    if not src.exists():
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {src}")

    with open(src, "r", encoding="utf-8") as f:
        payload = json.load(f)

    book = payload.get("book") if isinstance(payload, dict) else src.stem
    quotes = _load_quotes(payload)
    
    if not quotes:
        out = Path(output_json_path) if output_json_path else src
        _save_quotes(str(book or ""), [], out)
        return str(out)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
    smart_extractor = SmartQuoteExtractor()
    improved_quotes = []
    
    print(f"–£–ª—É—á—à–∞–µ–º {len(quotes)} —Ü–∏—Ç–∞—Ç...")
    
    for quote_data in tqdm(quotes, desc="–£–ª—É—á—à–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç"):
        original_text = quote_data.get("original", "")
        current_quote = quote_data.get("quote", "")
        page_num = quote_data.get("page")
        
        if not original_text or not current_quote:
            continue
            
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —Ü–∏—Ç–∞—Ç—É
        analyses = smart_extractor.analyze_paragraph(original_text, page_num)
        
        # –ò—â–µ–º –ª—É—á—à—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É
        best_analysis = None
        for analysis in analyses:
            if analysis.quality in [QuoteQuality.EXCELLENT, QuoteQuality.GOOD] and analysis.confidence > 0.6:
                if not best_analysis or analysis.confidence > best_analysis.confidence:
                    best_analysis = analysis
        
        if best_analysis:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Ü–∏—Ç–∞—Ç—É
            improved_quote = {
                **quote_data,
                "quote": best_analysis.text,
                "translated": best_analysis.text,  # –ü–æ–∫–∞ –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞
                "summary": best_analysis.summary,  # –ö—Ä–∞—Ç–∫–∞—è –∏–¥–µ—è
                "engaging": best_analysis.quality == QuoteQuality.EXCELLENT,
                "category": best_analysis.category,
                "meta": {
                    **quote_data.get("meta", {}),
                    "sentiment": best_analysis.sentiment,
                    "target_audience": best_analysis.target_audience,
                    "length": len(best_analysis.text),
                    "confidence": best_analysis.confidence,
                    "context_score": best_analysis.context_score,
                    "practical_value": best_analysis.practical_value,
                    "completeness": best_analysis.completeness,
                    "quote_type": best_analysis.quote_type.value,
                    "reasoning": best_analysis.reasoning,
                    "improved": True
                }
            }
            improved_quotes.append(improved_quote)
        else:
            # –û—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ü–∏—Ç–∞—Ç—É, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            improved_quote = {
                **quote_data,
                "meta": {
                    **quote_data.get("meta", {}),
                    "improved": False,
                    "reasoning": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª—É—á—à–µ–π –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã"
                }
            }
            improved_quotes.append(improved_quote)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_quotes = []
    seen_quotes = set()
    
    for quote in improved_quotes:
        quote_text = quote.get("quote", "").strip()
        if quote_text and quote_text not in seen_quotes:
            unique_quotes.append(quote)
            seen_quotes.add(quote_text)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    out = Path(output_json_path) if output_json_path else src
    _save_quotes(str(book or ""), unique_quotes, out)
    
    print(f"‚úÖ –£–ª—É—á—à–µ–Ω–æ {len(unique_quotes)} —Ü–∏—Ç–∞—Ç. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {out}")
    return str(out)


def deep_scan_with_gemini(pdf_path: str, output_json_path: Optional[str] = None) -> str:
    """
    üöÄ –ì–õ–£–ë–û–ö–û–ï –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –∫–Ω–∏–≥–∏ —Å –ø–æ–º–æ—â—å—é Gemini.

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï —Ü–µ–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ –∫–Ω–∏–≥–∏ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Gemini 2.5 Flash - –æ—á–µ–Ω—å –¥–µ—à–µ–≤–æ –∏ –±—ã—Å—Ç—Ä–æ!

    Args:
        pdf_path: –ü—É—Ç—å –∫ PDF –∫–Ω–∏–≥–µ
        output_json_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    print(f"\nüîß deep_scan_with_gemini() –∑–∞–ø—É—â–µ–Ω–∞")
    print(f"üìÅ PDF –ø—É—Ç—å: {pdf_path}")
    print(f"üíæ Output –ø—É—Ç—å: {output_json_path}")

    try:
        print("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ GeminiDeepExtractor...")
        extractor = GeminiDeepExtractor()
        print("‚úÖ Extractor —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")

        print("üöÄ –ó–∞–ø—É—Å–∫ extract_from_pdf_deep_scan()...")
        result = extractor.extract_from_pdf_deep_scan(pdf_path, output_json_path)
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        return result
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–ª—É–±–æ–∫–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return ""


