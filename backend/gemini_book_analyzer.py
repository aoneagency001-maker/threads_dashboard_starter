"""
–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–Ω–∏–≥ —Å –ø–æ–º–æ—â—å—é Google Gemini.
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Å–∞–π—Ç—ã –ø–æ –≥–ª–∞–≤–∞–º, –º–µ—Ç–æ–¥–∞–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.
"""

import os
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
import google.generativeai as genai
from dotenv import load_dotenv
import re

load_dotenv()


class GeminiBookAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–Ω–∏–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ Gemini AI"""

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–Ω—Å–∞–π—Ç–æ–≤
    CATEGORIES = [
        "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥",
        "–ø—Ä–æ–¥–∞–∂–∏",
        "–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è",
        "–º—ã—à–ª–µ–Ω–∏–µ",
        "–ª–∏–¥–µ—Ä—Å—Ç–≤–æ",
        "—Ñ–∏–Ω–∞–Ω—Å—ã",
        "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è",
        "–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"
    ]

    # –¢–∏–ø—ã –º–µ—Ç–æ–¥–æ–≤
    METHOD_TYPES = [
        "framework",      # –§—Ä–µ–π–º–≤–æ—Ä–∫/—Å–∏—Å—Ç–µ–º–∞
        "rule",          # –ü—Ä–∞–≤–∏–ª–æ/–ø—Ä–∏–Ω—Ü–∏–ø
        "technique",     # –¢–µ—Ö–Ω–∏–∫–∞/–º–µ—Ç–æ–¥
        "mistake",       # –û—à–∏–±–∫–∞/—á–µ–≥–æ –∏–∑–±–µ–≥–∞—Ç—å
        "case_study",    # –ö–µ–π—Å/–ø—Ä–∏–º–µ—Ä
        "exercise",      # –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ
        "insight"        # –ò–Ω—Å–∞–π—Ç/–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
    ]

    def __init__(self, api_key: str = None):
        """
        Args:
            api_key: Gemini API –∫–ª—é—á (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ .env)
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("Gemini API key not found. Add GEMINI_API_KEY to .env file")

        genai.configure(api_key=self.api_key)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        # –û—Ç–∫–ª—é—á–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª—é–±–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫–Ω–∏–≥
        from google.generativeai.types import HarmCategory, HarmBlockThreshold

        self.model = genai.GenerativeModel(
            'gemini-2.5-flash',
            safety_settings={
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }
        )

    def split_into_chapters(self, full_text: str) -> List[Dict[str, str]]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∫–Ω–∏–≥–∏ –Ω–∞ –≥–ª–∞–≤—ã.

        Returns:
            List[Dict]: [{chapter_num, title, content}, ...]
        """
        print("üìë –†–∞–∑–±–∏–≤–∞–µ–º –∫–Ω–∏–≥—É –Ω–∞ –≥–ª–∞–≤—ã...")

        # –ò—â–µ–º –º–∞—Ä–∫–µ—Ä—ã –≥–ª–∞–≤ (—Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã)
        chapter_patterns = [
            r'(?:^|\n)(?:–ì–ª–∞–≤–∞|–ì–õ–ê–í–ê|Chapter|CHAPTER)\s+(\d+|[IVX]+)[:\.\s]+([^\n]+)',
            r'(?:^|\n)(\d+)\.\s+([^\n]{10,100})\n',
            r'(?:^|\n)([IVX]+)\.\s+([^\n]{10,100})\n'
        ]

        chapters = []

        for pattern in chapter_patterns:
            matches = list(re.finditer(pattern, full_text, re.MULTILINE | re.IGNORECASE))
            if len(matches) >= 3:  # –ù–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≥–ª–∞–≤
                print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(matches)} –≥–ª–∞–≤ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É: {pattern[:30]}...")

                for i, match in enumerate(matches):
                    chapter_num = match.group(1)
                    chapter_title = match.group(2).strip()

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –≥–ª–∞–≤—ã (–¥–æ —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤—ã –∏–ª–∏ –¥–æ –∫–æ–Ω—Ü–∞)
                    start_pos = match.end()
                    end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
                    content = full_text[start_pos:end_pos].strip()

                    if len(content) > 100:  # –¢–æ–ª—å–∫–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã
                        chapters.append({
                            "chapter_num": chapter_num,
                            "title": chapter_title,
                            "content": content,
                            "length": len(content)
                        })

                if chapters:
                    break

        # –ï—Å–ª–∏ –≥–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –¥–µ–ª–∏–º –Ω–∞ —Ä–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏
        if not chapters:
            print("‚ö†Ô∏è –Ø–≤–Ω—ã–µ –≥–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –¥–µ–ª–∏–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ ~50k —Å–∏–º–≤–æ–ª–æ–≤...")
            chunk_size = 50000
            num_chunks = (len(full_text) + chunk_size - 1) // chunk_size

            for i in range(num_chunks):
                start = i * chunk_size
                end = min((i + 1) * chunk_size, len(full_text))
                chapters.append({
                    "chapter_num": str(i + 1),
                    "title": f"–ß–∞—Å—Ç—å {i + 1}",
                    "content": full_text[start:end],
                    "length": end - start
                })

        print(f"‚úÖ –í—Å–µ–≥–æ –≥–ª–∞–≤: {len(chapters)}")
        return chapters

    def extract_insights_from_chapter(
        self,
        chapter_content: str,
        chapter_title: str,
        max_chars: int = 100000
    ) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Å–∞–π—Ç—ã –∏–∑ –æ–¥–Ω–æ–π –≥–ª–∞–≤—ã —Å –ø–æ–º–æ—â—å—é Gemini.

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –∏–Ω—Å–∞–π—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è
        if len(chapter_content) > max_chars:
            chapter_content = chapter_content[:max_chars]

        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≥–ª–∞–≤—É –∫–Ω–∏–≥–∏ –∏ –∏–∑–≤–ª–µ–∫–∏ –í–°–ï —Ü–µ–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã.

üìñ –ì–õ–ê–í–ê: {chapter_title}

üéØ –ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –º—ã—Å–ª–∏, –º–µ—Ç–æ–¥—ã, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏, –ø—Ä–∞–≤–∏–ª–∞, –ø—Ä–∏–º–µ—Ä—ã.

üìä –ß–¢–û –ò–ó–í–õ–ï–ö–ê–¢–¨:
1. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏
2. –§—Ä–µ–π–º–≤–æ—Ä–∫–∏ –∏ —Å–∏—Å—Ç–µ–º—ã
3. –ü—Ä–∞–≤–∏–ª–∞ –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã
4. –ü—Ä–∏–º–µ—Ä—ã –∏ –∫–µ–π—Å—ã
5. –û—à–∏–±–∫–∏ –∏ —á—Ç–æ –∏–∑–±–µ–≥–∞—Ç—å
6. –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã

‚ùå –ß–¢–û –ù–ï –ò–ó–í–õ–ï–ö–ê–¢–¨:
- –í–≤–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏ —Å–≤—è–∑–∫–∏
- –°—Å—ã–ª–∫–∏ –∏ —Å–Ω–æ—Å–∫–∏
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- –ü–æ–≤—Ç–æ—Ä—ã

üìè –§–û–†–ú–ê–¢ –ö–ê–ñ–î–û–ì–û –ò–ù–°–ê–ô–¢–ê:
- –î–ª–∏–Ω–∞: 50-500 —Å–∏–º–≤–æ–ª–æ–≤
- –ó–∞–∫–æ–Ω—á–µ–Ω–Ω–∞—è –º—ã—Å–ª—å
- –ü–æ–Ω—è—Ç–Ω–∞ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å

üè∑Ô∏è –ö–ê–¢–ï–ì–û–†–ò–ò:
{', '.join(self.CATEGORIES)}

üîß –¢–ò–ü–´ –ú–ï–¢–û–î–û–í:
- framework: —Ñ—Ä–µ–π–º–≤–æ—Ä–∫/—Å–∏—Å—Ç–µ–º–∞
- rule: –ø—Ä–∞–≤–∏–ª–æ/–ø—Ä–∏–Ω—Ü–∏–ø
- technique: —Ç–µ—Ö–Ω–∏–∫–∞/–º–µ—Ç–æ–¥
- mistake: –æ—à–∏–±–∫–∞/—á–µ–≥–æ –∏–∑–±–µ–≥–∞—Ç—å
- case_study: –∫–µ–π—Å/–ø—Ä–∏–º–µ—Ä
- exercise: —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ
- insight: –∏–Ω—Å–∞–π—Ç/–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ

üìù –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê - —Å—Ç—Ä–æ–≥–æ JSON:
{{
  "insights": [
    {{
      "text": "–¢–µ–∫—Å—Ç –∏–Ω—Å–∞–π—Ç–∞ (–≥–æ—Ç–æ–≤—ã–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)",
      "category": "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥|–ø—Ä–æ–¥–∞–∂–∏|...",
      "method_type": "framework|rule|technique|...",
      "title": "–ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞/–∏–¥–µ–∏",
      "description": "–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö",
      "practical_value": 0.0-1.0,
      "actionable": true/false
    }},
    ...
  ]
}}

–¢–ï–ö–°–¢ –ì–õ–ê–í–´:
{chapter_content[:40000]}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
"""

        try:
            print(f"   ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–∞–≤—É '{chapter_title}'...")

            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=16384,
                )
            )

            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            response_text = response.text.strip()

            # –£–±–∏—Ä–∞–µ–º markdown –æ–±–µ—Ä—Ç–∫—É
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            elif response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]

            response_text = response_text.strip()

            data = json.loads(response_text)
            insights = data.get("insights", [])

            print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(insights)} –∏–Ω—Å–∞–π—Ç–æ–≤")

            return insights

        except json.JSONDecodeError as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            print(f"   –û—Ç–≤–µ—Ç: {response_text[:300]}...")
            return []
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
            return []

    def analyze_full_book(
        self,
        full_text: str,
        book_title: str
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–Ω–∏–≥–∏: —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –≥–ª–∞–≤—ã + –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤.

        Returns:
            Dict —Å –ø–æ–ª–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∫–Ω–∏–≥–∏ –∏ –≤—Å–µ–º–∏ –∏–Ω—Å–∞–π—Ç–∞–º–∏
        """
        print(f"\n{'='*60}")
        print(f"üìö –ê–ù–ê–õ–ò–ó –ö–ù–ò–ì–ò: {book_title}")
        print(f"{'='*60}\n")

        # –®–∞–≥ 1: –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥–ª–∞–≤—ã
        chapters = self.split_into_chapters(full_text)

        # –®–∞–≥ 2: –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Å–∞–π—Ç—ã –∏–∑ –∫–∞–∂–¥–æ–π –≥–ª–∞–≤—ã
        print(f"\nüîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ {len(chapters)} –≥–ª–∞–≤...\n")

        all_insights = []
        chapters_data = []

        for idx, chapter in enumerate(chapters, 1):
            print(f"üìñ –ì–ª–∞–≤–∞ {idx}/{len(chapters)}: {chapter['title']}")

            insights = self.extract_insights_from_chapter(
                chapter['content'],
                chapter['title']
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã –∫ –∫–∞–∂–¥–æ–º—É –∏–Ω—Å–∞–π—Ç—É
            for insight in insights:
                insight['chapter_num'] = chapter['chapter_num']
                insight['chapter_title'] = chapter['title']
                insight['length'] = len(insight.get('text', ''))

            chapters_data.append({
                "chapter_num": chapter['chapter_num'],
                "title": chapter['title'],
                "insights_count": len(insights),
                "content_length": chapter['length']
            })

            all_insights.extend(insights)
            print()

        # –®–∞–≥ 3: –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –º–µ—Ç–æ–¥–∞–º
        by_category = {}
        by_method = {}

        for insight in all_insights:
            # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            cat = insight.get('category', '–¥—Ä—É–≥–æ–µ')
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(insight)

            # –ü–æ —Ç–∏–ø—É –º–µ—Ç–æ–¥–∞
            method = insight.get('method_type', 'insight')
            if method not in by_method:
                by_method[method] = []
            by_method[method].append(insight)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        result = {
            "book_title": book_title,
            "total_chapters": len(chapters),
            "total_insights": len(all_insights),
            "analysis_method": "gemini_2.5_flash",

            # –í—Å–µ –∏–Ω—Å–∞–π—Ç—ã (—Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏)
            "all_insights": all_insights,

            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ –≥–ª–∞–≤–∞–º
            "chapters": chapters_data,

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            "by_category": {
                cat: {
                    "count": len(items),
                    "insights": items
                }
                for cat, items in by_category.items()
            },

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º
            "by_method": {
                method: {
                    "count": len(items),
                    "insights": items
                }
                for method, items in by_method.items()
            },

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            "statistics": {
                "avg_insight_length": sum(i['length'] for i in all_insights) // len(all_insights) if all_insights else 0,
                "actionable_count": len([i for i in all_insights if i.get('actionable')]),
                "high_value_count": len([i for i in all_insights if i.get('practical_value', 0) >= 0.7])
            }
        }

        print(f"\n{'='*60}")
        print(f"‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        print(f"{'='*60}")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –ì–ª–∞–≤: {len(chapters)}")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∏–Ω—Å–∞–π—Ç–æ–≤: {len(all_insights)}")
        print(f"   ‚Ä¢ –ü—Ä–∞–∫—Ç–∏—á–Ω—ã—Ö: {result['statistics']['actionable_count']}")
        print(f"   ‚Ä¢ –í—ã—Å–æ–∫–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {result['statistics']['high_value_count']}")
        print(f"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(by_category)}")
        print(f"{'='*60}\n")

        return result

    def analyze_pdf(
        self,
        pdf_path: str,
        output_json_path: Optional[str] = None
    ) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç PDF –∫–Ω–∏–≥—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.

        Returns:
            –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        from . import parser as book_parser

        print(f"üìñ –ó–∞–≥—Ä—É–∑–∫–∞ PDF: {pdf_path}")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        pages = book_parser.extract_pages_from_pdf(pdf_path)
        full_text = "\n\n".join([
            book_parser.clean_text(page)
            for page in pages
        ])

        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü, {len(full_text):,} —Å–∏–º–≤–æ–ª–æ–≤")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        book_title = Path(pdf_path).stem
        result = self.analyze_full_book(full_text, book_title)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        if output_json_path:
            out_path = Path(output_json_path)
        else:
            base_dir = Path(__file__).resolve().parents[1]
            quotes_dir = base_dir / "data" / "quotes"
            quotes_dir.mkdir(parents=True, exist_ok=True)
            out_path = quotes_dir / f"{book_title.replace(' ', '-')}_gemini_analysis.json"

        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {out_path}\n")

        return str(out_path)
