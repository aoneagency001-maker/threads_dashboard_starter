"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ LLM (Claude, OpenAI)
"""
import os
import json
from typing import Optional, Dict, Any, List
from enum import Enum

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass


class LLMProvider(Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã LLM"""
    CLAUDE = "claude"
    OPENAI = "openai"
    AUTO = "auto"  # –ê–≤—Ç–æ–≤—ã–±–æ—Ä (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: Claude -> OpenAI)


class LLMClient:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM"""

    def __init__(self, provider: LLMProvider = LLMProvider.AUTO):
        self.provider = provider
        self.claude_client = None
        self.openai_client = None

        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á–∏
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã
        if anthropic_key:
            try:
                from anthropic import Anthropic
                self.claude_client = Anthropic(api_key=anthropic_key)
                print("‚úÖ Claude API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except ImportError:
                print("‚ö†Ô∏è –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'anthropic' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install anthropic")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Claude: {e}")

        if openai_key:
            try:
                from openai import OpenAI
                self.openai_client = OpenAI(api_key=openai_key)
                print("‚úÖ OpenAI API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except ImportError:
                print("‚ö†Ô∏è –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'openai' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        if provider == LLMProvider.AUTO:
            if self.claude_client:
                self.active_provider = LLMProvider.CLAUDE
                print("üéØ –í—ã–±—Ä–∞–Ω –ø—Ä–æ–≤–∞–π–¥–µ—Ä: Claude (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π)")
            elif self.openai_client:
                self.active_provider = LLMProvider.OPENAI
                print("üéØ –í—ã–±—Ä–∞–Ω –ø—Ä–æ–≤–∞–π–¥–µ—Ä: OpenAI (fallback)")
            else:
                self.active_provider = None
                print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤!")
        else:
            self.active_provider = provider
            if provider == LLMProvider.CLAUDE and not self.claude_client:
                print("‚ö†Ô∏è Claude API –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ —É–∫–∞–∑–∞–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
            elif provider == LLMProvider.OPENAI and not self.openai_client:
                print("‚ö†Ô∏è OpenAI API –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ —É–∫–∞–∑–∞–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")

    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
        return self.claude_client is not None or self.openai_client is not None

    def get_model_name(self, model_type: str = "default") -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        if self.active_provider == LLMProvider.CLAUDE:
            if model_type == "fast":
                return "claude-3-haiku-20240307"  # –ë—ã—Å—Ç—Ä–∞—è –∏ –¥–µ—à—ë–≤–∞—è
            elif model_type == "smart":
                return "claude-3-5-sonnet-20241022"  # –£–º–Ω–∞—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            else:
                return "claude-3-5-sonnet-20241022"

        elif self.active_provider == LLMProvider.OPENAI:
            if model_type == "fast":
                return "gpt-4o-mini"
            elif model_type == "smart":
                return "gpt-4o"
            else:
                return "gpt-4o-mini"

        return None

    def chat(
        self,
        system_prompt: str,
        user_prompt: str,
        model_type: str = "default",
        temperature: float = 0.2,
        max_tokens: int = 4096,
        response_format: Optional[str] = None
    ) -> Optional[str]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—â–µ–Ω–∏—è —Å LLM

        Args:
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            user_prompt: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ ('default', 'fast', 'smart')
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0-1)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            response_format: –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (None –∏–ª–∏ 'json')

        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not self.is_available():
            print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤")
            return None

        model = self.get_model_name(model_type)

        try:
            # Claude API
            if self.active_provider == LLMProvider.CLAUDE and self.claude_client:
                response = self.claude_client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ]
                )
                return response.content[0].text

            # OpenAI API
            elif self.active_provider == LLMProvider.OPENAI and self.openai_client:
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]

                kwargs = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }

                # –î–æ–±–∞–≤–ª—è–µ–º JSON mode —Ç–æ–ª—å–∫–æ –¥–ª—è OpenAI
                if response_format == "json":
                    kwargs["response_format"] = {"type": "json_object"}

                response = self.openai_client.chat.completions.create(**kwargs)
                return response.choices[0].message.content

            else:
                print(f"‚ùå –ü—Ä–æ–≤–∞–π–¥–µ—Ä {self.active_provider} –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
                return None

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ {self.active_provider.value}: {e}")
            return None

    def parse_json_response(self, response: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç JSON –æ—Ç–≤–µ—Ç –æ—Ç LLM"""
        if not response:
            return None

        try:
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å
            return json.loads(response)
        except json.JSONDecodeError:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –≤ —Ç–µ–∫—Å—Ç–µ
            import re
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except json.JSONDecodeError:
                    pass

            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ markdown –±–ª–æ–∫–∞
            json_block = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_block:
                try:
                    return json.loads(json_block.group(1))
                except json.JSONDecodeError:
                    pass

            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç: {response[:200]}...")
            return None


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å (—Å–∏–Ω–≥–ª—Ç–æ–Ω)
_global_client: Optional[LLMClient] = None


def get_llm_client(provider: LLMProvider = LLMProvider.AUTO) -> LLMClient:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å LLM –∫–ª–∏–µ–Ω—Ç–∞"""
    global _global_client
    if _global_client is None:
        _global_client = LLMClient(provider)
    return _global_client


def reset_llm_client():
    """–°–±—Ä–æ—Å–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)"""
    global _global_client
    _global_client = None


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
    client = get_llm_client(LLMProvider.AUTO)

    if client.is_available():
        # –¢–µ—Å—Ç —Å –ø—Ä–æ—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
        response = client.chat(
            system_prompt="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É.",
            user_prompt="–î–∞–π –∫—Ä–∞—Ç–∫–∏–π —Å–æ–≤–µ—Ç –ø–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –ø—Ä–æ–¥–∞–∂ (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)",
            model_type="smart",
            temperature=0.3
        )

        print("\nüìù –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
        print(response)

        # –¢–µ—Å—Ç —Å JSON –æ—Ç–≤–µ—Ç–æ–º
        json_response = client.chat(
            system_prompt="–¢—ã –≤–æ–∑–≤—Ä–∞—â–∞–µ—à—å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON.",
            user_prompt='–í–µ—Ä–Ω–∏ JSON —Å –ø–æ–ª—è–º–∏: {"—Å–æ–≤–µ—Ç": "—Ç–µ–∫—Å—Ç —Å–æ–≤–µ—Ç–∞", "–∫–∞—Ç–µ–≥–æ—Ä–∏—è": "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥"}',
            model_type="smart",
            temperature=0.2,
            response_format="json"
        )

        print("\nüìä JSON –æ—Ç–≤–µ—Ç:")
        parsed = client.parse_json_response(json_response)
        print(json.dumps(parsed, ensure_ascii=False, indent=2))
    else:
        print("‚ùå LLM –∫–ª–∏–µ–Ω—Ç –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á–∏ –≤ .env —Ñ–∞–π–ª–µ")
