"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –∫–Ω–∏–≥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Claude API –∏ —Å—Ç—Ä–æ–≥–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
"""
import fitz  # PyMuPDF
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm

from .llm_client import get_llm_client
from .prompts import (
    get_extract_quotes_prompts,
    get_translate_prompts,
    get_infer_topic_prompts
)
from .quote_validator import QuoteValidator

# –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–∞–Ω–Ω—ã—Ö
BASE_DIR = Path(__file__).resolve().parents[1]
BOOKS_DIR = BASE_DIR / "data" / "books"
QUOTES_DIR = BASE_DIR / "data" / "quotes"
BOOKS_DIR.mkdir(parents=True, exist_ok=True)
QUOTES_DIR.mkdir(parents=True, exist_ok=True)


# ============================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================

def _slugify_filename(file_path: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞"""
    name = Path(file_path).stem
    name = re.sub(r"[^\w\-]+", "-", name, flags=re.IGNORECASE)
    name = re.sub(r"-+", "-", name).strip("-")
    return name.lower() or "book"


def _quotes_output_path_for_book(file_path: str) -> Path:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ü–∏—Ç–∞—Ç"""
    slug = _slugify_filename(file_path)
    return QUOTES_DIR / f"{slug}.json"


def _infer_author_and_topic_from_name(name: str) -> Dict[str, str]:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∞–≤—Ç–æ—Ä–∞ –∏ —Ç–µ–º—É –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞)"""
    name_low = name.lower()
    author = ""
    topic = ""

    # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –∞–≤—Ç–æ—Ä—ã –∏ –∫–Ω–∏–≥–∏
    known_books = {
        ("brunson", "dotcom secrets", "dot com secrets", "traffic secrets", "expert secrets"): {
            "author": "Russell Brunson",
            "topic": "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ –≤–æ—Ä–æ–Ω–∫–∏ –ø—Ä–æ–¥–∞–∂"
        },
        ("cialdini", "influence", "–≤–ª–∏—è–Ω–∏–µ"): {
            "author": "Robert Cialdini",
            "topic": "–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è –≤–ª–∏—è–Ω–∏—è"
        },
        ("kotler", "marketing"): {
            "author": "Philip Kotler",
            "topic": "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥"
        },
    }

    for keywords, info in known_books.items():
        if any(k in name_low for k in keywords):
            return info

    # –û–±—â–∏–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–µ–º–∞–º
    if any(k in name_low for k in ["sales", "–ø—Ä–æ–¥–∞–∂", "selling"]):
        topic = "–ø—Ä–æ–¥–∞–∂–∏"
    elif any(k in name_low for k in ["marketing", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "ads", "advertis"]):
        topic = "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥"
    elif any(k in name_low for k in ["business", "–±–∏–∑–Ω–µ—Å", "entrepreneur"]):
        topic = "–±–∏–∑–Ω–µ—Å –∏ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å—Ç–≤–æ"
    elif any(k in name_low for k in ["psychology", "–ø—Å–∏—Ö–æ–ª–æ–≥"]):
        topic = "–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è"

    return {"author": author, "topic": topic}


# ============================================
# –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–ï–ö–°–¢–ê –ò–ó PDF
# ============================================

def extract_pages_from_pdf(file_path: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ –∏–∑ PDF"""
    pages: List[str] = []
    doc = fitz.open(file_path)

    for page in doc:
        blocks = page.get_text("blocks")
        chunks: List[str] = []

        for block in blocks:
            if len(block) >= 5 and isinstance(block[4], str):
                chunks.append(block[4].strip())

        pages.append("\n".join(chunks))

    return pages


def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Ç–∞–±—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
    text = re.sub(r"[\t\r]+", " ", text)
    # –£–±–∏—Ä–∞–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"\u00a0", " ", text)
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"\s+", " ", text).strip()
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã –ø–æ—Å–ª–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    text = re.sub(r"([.!?])\s+", r"\1\n", text)

    return text


# ============================================
# –†–ê–ë–û–¢–ê –° LLM
# ============================================

def infer_topic_via_llm(sample_text: str, fallback_topic: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—É –∫–Ω–∏–≥–∏ —á–µ—Ä–µ–∑ LLM"""
    llm_client = get_llm_client()

    if not llm_client.is_available():
        return fallback_topic

    system_prompt, user_prompt = get_infer_topic_prompts(sample_text)

    try:
        response = llm_client.chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            model_type="fast",  # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–∏
            temperature=0.0,
            response_format="json"
        )

        if response:
            data = llm_client.parse_json_response(response)
            if data and "topic" in data:
                confidence = data.get("confidence", 0.0)
                if confidence >= 0.7:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–≤–µ—Ä–µ–Ω—ã
                    return data["topic"].strip().lower()

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º—ã —á–µ—Ä–µ–∑ LLM: {e}")

    return fallback_topic


def extract_quotes_from_chunk(
    chunk: str,
    topic: str,
    audience: str = "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—è–º –∏ –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∞–º"
) -> List[Dict[str, Any]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–∏—Ç–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞"""
    llm_client = get_llm_client()

    if not llm_client.is_available():
        # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–Ω—É —Ü–∏—Ç–∞—Ç—É
        return [{
            "summary": chunk[:100] + "..." if len(chunk) > 100 else chunk,
            "quote": chunk[:200],
            "category": "general",
            "style": "insight",
            "target_audience": "general",
            "practical_value": 0.5
        }]

    system_prompt, user_prompt = get_extract_quotes_prompts(chunk, topic, audience)

    try:
        response = llm_client.chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            model_type="smart",  # –£–º–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            temperature=0.3,
            max_tokens=2048,
            response_format="json"
        )

        if not response:
            return []

        data = llm_client.parse_json_response(response)
        if not data or "quotes" not in data:
            return []

        return data["quotes"]

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–∏—Ç–∞—Ç: {e}")
        return []


def translate_text(text: str, topic: str = "–±–∏–∑–Ω–µ—Å") -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
    if not text or not text.strip():
        return text

    llm_client = get_llm_client()

    if not llm_client.is_available():
        return text

    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —É–∂–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º?
    russian_chars = len(re.findall(r'[–∞-—è—ë–ê-–Ø–Å]', text))
    total_chars = len(re.findall(r'[a-zA-Z–∞-—è—ë–ê-–Ø–Å]', text))

    if total_chars > 0 and russian_chars / total_chars > 0.5:
        # –£–∂–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        return text

    system_prompt, user_prompt = get_translate_prompts(text, topic)

    try:
        response = llm_client.chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            model_type="smart",  # –£–º–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞
            temperature=0.2
        )

        if response and response.strip():
            return response.strip()

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")

    return text


# ============================================
# –†–ê–ó–ë–ò–í–ö–ê –¢–ï–ö–°–¢–ê –ù–ê –ü–ê–†–ê–ì–†–ê–§–´
# ============================================

def chunk_paragraphs(text: str, max_sentences: int = 5) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
    paragraphs = [p.strip() for p in re.split(r"\n{2,}", text) if p.strip()]

    chunks: List[str] = []

    for paragraph in paragraphs:
        # –†–∞–∑–±–∏–≤–∞–µ–º –∞–±–∑–∞—Ü –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = [s.strip() for s in re.split(r"(?<=[.!?])\s+", paragraph) if s.strip()]

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ max_sentences –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        for i in range(0, len(sentences), max_sentences):
            chunk = " ".join(sentences[i:i + max_sentences])
            if len(chunk) >= 80:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
                chunks.append(chunk)

    return chunks


# ============================================
# –û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù
# ============================================

def extract_insightful_quotes(
    file_path: str,
    min_total: int = 20,
    max_total: int = 50
) -> List[Dict[str, Any]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ª—É—á—à–∏–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –∫–Ω–∏–≥–∏

    Args:
        file_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
        min_total: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ç–∞—Ç
        max_total: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ç–∞—Ç

    Returns:
        –°–ø–∏—Å–æ–∫ —Ü–∏—Ç–∞—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    print(f"\nüìö –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–∏–≥–∏: {Path(file_path).name}")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    pages = extract_pages_from_pdf(file_path)
    print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏
    book_name = Path(file_path).stem
    meta = _infer_author_and_topic_from_name(book_name)

    # –£—Ç–æ—á–Ω—è–µ–º —Ç–µ–º—É —á–µ—Ä–µ–∑ LLM
    sample_text = clean_text(pages[0]) if pages else ""
    topic = infer_topic_via_llm(sample_text, meta.get("topic", "–±–∏–∑–Ω–µ—Å"))
    author = meta.get("author", "")

    print(f"üéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ç–µ–º–∞: {topic}")
    if author:
        print(f"‚úçÔ∏è –ê–≤—Ç–æ—Ä: {author}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
    validator = QuoteValidator()

    # –°–æ–±–∏—Ä–∞–µ–º —Ü–∏—Ç–∞—Ç—ã
    collected: List[Dict[str, Any]] = []
    seen_quotes = set()

    print(f"\nüîç –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–∏—Ç–∞—Ç—ã...")

    for page_idx, page_text in enumerate(tqdm(pages, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"), start=1):
        if len(collected) >= max_total:
            break

        cleaned_page = clean_text(page_text)

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        chunks = chunk_paragraphs(cleaned_page, max_sentences=5)

        for chunk in chunks:
            if len(collected) >= max_total:
                break

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–∏—Ç–∞—Ç—ã –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
            quotes_from_chunk = extract_quotes_from_chunk(chunk, topic)

            for quote_data in quotes_from_chunk:
                quote_text = quote_data.get("quote", "").strip()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                if quote_text in seen_quotes:
                    continue

                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–∏—Ç–∞—Ç—ã
                is_valid, validation_result = validator.validate_quote(quote_text, chunk)

                if is_valid:
                    collected.append({
                        "page": page_idx,
                        "original": chunk,
                        "summary": quote_data.get("summary", ""),
                        "quote": quote_text,
                        "category": quote_data.get("category", "general"),
                        "style": quote_data.get("style", "insight"),
                        "target_audience": quote_data.get("target_audience", "general"),
                        "validation": validation_result
                    })
                    seen_quotes.add(quote_text)

                if len(collected) >= max_total:
                    break

    print(f"\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ü–∏—Ç–∞—Ç: {len(collected)}")

    # –ï—Å–ª–∏ –º–∞–ª–æ, —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
    if len(collected) < min_total:
        print(f"‚ö†Ô∏è –ú–∞–ª–æ —Ü–∏—Ç–∞—Ç ({len(collected)}), —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")

        for page_idx, page_text in enumerate(pages, start=1):
            if len(collected) >= min_total:
                break

            cleaned_page = clean_text(page_text)
            chunks = chunk_paragraphs(cleaned_page)

            for chunk in chunks:
                if len(collected) >= min_total:
                    break

                if chunk not in seen_quotes and len(chunk) >= 80:
                    # –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
                    basic_valid, _ = validator._basic_validation(chunk[:200])

                    if basic_valid:
                        collected.append({
                            "page": page_idx,
                            "original": chunk,
                            "summary": "",
                            "quote": chunk[:200],
                            "category": "general",
                            "style": "insight",
                            "target_audience": "general",
                            "validation": {"level": "basic_only"}
                        })
                        seen_quotes.add(chunk[:200])

    return collected[:max_total]


def save_quotes_file(
    book_title: str,
    quotes: List[Dict[str, Any]],
    output_path: str
) -> int:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ü–∏—Ç–∞—Ç—ã –≤ JSON —Ñ–∞–π–ª"""
    payload = {"book": book_title, "quotes": quotes}
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    return len(quotes)


def process_book(
    file_path: str,
    output_path: Optional[str] = None,
    force: bool = True
) -> str:
    """
    –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–∏–≥–∏: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, –ø–µ—Ä–µ–≤–æ–¥, –≤–∞–ª–∏–¥–∞—Ü–∏—è, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

    Args:
        file_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        force: –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª

    Returns:
        –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    # –ü—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    out_path = output_path or str(_quotes_output_path_for_book(file_path))

    if not force and Path(out_path).exists():
        print(f"‚ÑπÔ∏è –§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {out_path}")
        return out_path

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–∏—Ç–∞—Ç—ã
    quotes_raw = extract_insightful_quotes(file_path)

    if not quotes_raw:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ü–∏—Ç–∞—Ç—ã")
        return ""

    # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Ü–∏—Ç–∞—Ç—ã
    book_title = Path(file_path).stem
    book_topic = quotes_raw[0].get("category", "–±–∏–∑–Ω–µ—Å") if quotes_raw else "–±–∏–∑–Ω–µ—Å"

    final_quotes: List[Dict[str, Any]] = []

    print(f"\nüåê –ü–µ—Ä–µ–≤–æ–¥ —Ü–∏—Ç–∞—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π...")

    for item in tqdm(quotes_raw, desc="–ü–µ—Ä–µ–≤–æ–¥", unit="—Ü–∏—Ç–∞—Ç–∞"):
        quote_text = item.get("quote", "")
        translated = translate_text(quote_text, book_topic)

        final_quotes.append({
            "page": item.get("page"),
            "original": item.get("original", ""),
            "summary": item.get("summary", ""),
            "quote": quote_text,
            "translated": translated,
            "engaging": True,
            "category": item.get("category", "general"),
            "style": item.get("style", "insight"),
            "meta": {
                "sentiment": "practical",
                "target_audience": item.get("target_audience", "general"),
                "length": len(quote_text),
                "validation_level": item.get("validation", {}).get("validation_level", "basic")
            }
        })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    count = save_quotes_file(book_title, final_quotes, out_path)

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {count} —Ü–∏—Ç–∞—Ç")
    print(f"üìÅ –§–∞–π–ª: {out_path}")

    return out_path


# ============================================
# –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        pdf_path = sys.argv[1]
    else:
        pdf_path = str(BOOKS_DIR / "DotCom Secrets PDF.pdf")

    if Path(pdf_path).exists():
        process_book(pdf_path)
    else:
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
        print(f"üí° –ü–æ–ª–æ–∂–∏—Ç–µ PDF —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {BOOKS_DIR}")
